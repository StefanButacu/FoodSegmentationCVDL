{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models.unet_from_scratch.UNET_MODEL_FROM_SCRATCH import UNET_FROM_SCRATCH\n",
    "from models.unet_from_scratch.utils import get_loaders, save_checkpoint, print_measurements, save_predictions_as_imgs, \\\n",
    "    load_checkpoint\n",
    "from models.unet_from_scratch.visual import DatasetViewer\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 7\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_HEIGHT = 240\n",
    "IMAGE_WIDTH = 240\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "\n",
    "TRAIN_IMG_DIR = 'E:\\CVDL\\MyApp\\models\\data\\images'\n",
    "TRAIN_MASK_DIR = 'E:\\CVDL\\MyApp\\models\\data\\labels'\n",
    "VAL_IMG_DIR = 'E:\\CVDL\\MyApp\\models\\data\\\\validationimages'\n",
    "VAL_MASK_DIR = 'E:\\CVDL\\MyApp\\models\\data\\\\validationlabels'\n",
    "\n",
    "train_transfrom = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Rotate(limit=35, p=1.0),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.Normalize(mean=[0.0, 0.0, 0.0],\n",
    "                    std=[1.0, 1.0, 1.0],\n",
    "                    max_pixel_value=255.0\n",
    "                    ),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transfrom = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Normalize(mean=[0.0, 0.0, 0.0],\n",
    "                    std=[1.0, 1.0, 1.0],\n",
    "                    max_pixel_value=255.0\n",
    "                    ),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_loaders(TRAIN_IMG_DIR,\n",
    "                                           TRAIN_MASK_DIR,\n",
    "                                           VAL_IMG_DIR,\n",
    "                                           VAL_MASK_DIR,\n",
    "                                           BATCH_SIZE,\n",
    "                                           train_transfrom,\n",
    "                                           val_transfrom,\n",
    "                                           NUM_WORKERS,\n",
    "                                           PIN_MEMORY\n",
    "                                           )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
    "    \"\"\"\n",
    "    :param loader:\n",
    "    :param model:\n",
    "    :param optimizer: - how the network will be updated based on the loss function\n",
    "    :param loss_fn: - quantity that will be minimized during training\n",
    "    :param scaler:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    loop = tqdm(loader)\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.unsqueeze(1).to(device=DEVICE)\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = model(data)\n",
    "            loss = loss_fn(pred, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UNET FROM SCRATCH\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\CVDL\\MyApp\\venv\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/94 [00:00<?, ?it/s]E:\\CVDL\\MyApp\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "100%|██████████| 94/94 [24:04<00:00, 15.37s/it, loss=0.415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>saving checkpoint to check_unet_scratch.pth.tar\n",
      "printing measure\n",
      "Got 70147379/86400000 with acc 81.19\n",
      "Got dice_score:0.8357783555984497 \n",
      "Got ioc1:0.41788917779922485 \n",
      "Epoch 2 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [23:56<00:00, 15.28s/it, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>saving checkpoint to check_unet_scratch.pth.tar\n",
      "printing measure\n",
      "Got 71081699/86400000 with acc 82.27\n",
      "Got dice_score:0.8455772995948792 \n",
      "Got ioc1:0.4227886497974396 \n",
      "Epoch 3 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [23:54<00:00, 15.26s/it, loss=0.368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>saving checkpoint to check_unet_scratch.pth.tar\n",
      "printing measure\n",
      "Got 72207125/86400000 with acc 83.57\n",
      "Got dice_score:0.8583813905715942 \n",
      "Got ioc1:0.4291906952857971 \n",
      "Epoch 4 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [23:49<00:00, 15.20s/it, loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>saving checkpoint to check_unet_scratch.pth.tar\n",
      "printing measure\n",
      "Got 72670187/86400000 with acc 84.11\n",
      "Got dice_score:0.8571197986602783 \n",
      "Got ioc1:0.42855989933013916 \n",
      "Epoch 5 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [24:07<00:00, 15.40s/it, loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>saving checkpoint to check_unet_scratch.pth.tar\n",
      "printing measure\n",
      "Got 73325568/86400000 with acc 84.87\n",
      "Got dice_score:0.8702993392944336 \n",
      "Got ioc1:0.4351496696472168 \n",
      "Epoch 6 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [25:52<00:00, 16.52s/it, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>saving checkpoint to check_unet_scratch.pth.tar\n",
      "printing measure\n",
      "Got 74108840/86400000 with acc 85.77\n",
      "Got dice_score:0.8761371970176697 \n",
      "Got ioc1:0.43806859850883484 \n",
      "Epoch 7 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [24:45<00:00, 15.80s/it, loss=0.319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>saving checkpoint to check_unet_scratch.pth.tar\n",
      "printing measure\n",
      "Got 74348951/86400000 with acc 86.05\n",
      "Got dice_score:0.8792921900749207 \n",
      "Got ioc1:0.4396460950374603 \n",
      "Saving images to.... unet_from_scratch_saved_photos/\n",
      "Finish saving images...\n",
      "End unet training\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "model = UNET_FROM_SCRATCH(in_channels=3, out_channels=1).to(DEVICE)\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # applies sigmoid on output\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1} / {NUM_EPOCHS}\")\n",
    "    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
    "\n",
    "    # save model\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict()\n",
    "    }\n",
    "    save_checkpoint(checkpoint, 'check_unet_scratch.pth.tar')\n",
    "    print_measurements(train_loader, model, device=DEVICE)\n",
    "\n",
    "    # check acc\n",
    "    # print examples\n",
    "    if epoch == NUM_EPOCHS -1:\n",
    "        save_predictions_as_imgs(train_loader, model, folder=\"unet_from_scratch_saved_photos/\", device=DEVICE)\n",
    "\n",
    "print(\"End unet training\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation UNET_FROM_SCRATCH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "# from tqdm import tqdm\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from models.unet_from_scratch.utils import get_loaders, save_checkpoint, print_measurements, save_predictions_as_imgs, \\\n",
    "#     load_checkpoint\n",
    "#\n",
    "# LEARNING_RATE = 1e-4\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# BATCH_SIZE = 16\n",
    "# NUM_EPOCHS = 10\n",
    "# NUM_WORKERS = 2\n",
    "# IMAGE_HEIGHT = 160\n",
    "# IMAGE_WIDTH = 240\n",
    "# PIN_MEMORY = True\n",
    "# LOAD_MODEL = True\n",
    "#\n",
    "# TRAIN_IMG_DIR = 'E:\\CVDL\\MyApp\\models\\data\\\\images'\n",
    "# TRAIN_MASK_DIR = 'E:\\CVDL\\MyApp\\models\\data\\labels'\n",
    "# VAL_IMG_DIR = 'E:\\CVDL\\MyApp\\models\\data\\\\validationimages'\n",
    "# VAL_MASK_DIR = 'E:\\CVDL\\MyApp\\models\\data\\\\validationlabels'\n",
    "#\n",
    "# train_transfrom = A.Compose(\n",
    "#     [\n",
    "#         A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "#         A.Rotate(limit=35, p=1.0),\n",
    "#         A.HorizontalFlip(p=0.5),\n",
    "#         A.VerticalFlip(p=0.1),\n",
    "#         A.Normalize(mean=[0.0, 0.0, 0.0],\n",
    "#                     std=[1.0, 1.0, 1.0],\n",
    "#                     max_pixel_value=255.0\n",
    "#                     ),\n",
    "#         ToTensorV2()\n",
    "#     ]\n",
    "# )\n",
    "#\n",
    "# val_transfrom = A.Compose(\n",
    "#     [\n",
    "#         A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "#         A.Normalize(mean=[0.0, 0.0, 0.0],\n",
    "#                     std=[1.0, 1.0, 1.0],\n",
    "#                     max_pixel_value=255.0\n",
    "#                     ),\n",
    "#         ToTensorV2()\n",
    "#     ]\n",
    "# )\n",
    "# train_loader, val_loader = get_loaders(TRAIN_IMG_DIR,\n",
    "#                                        TRAIN_MASK_DIR,\n",
    "#                                        VAL_IMG_DIR,\n",
    "#                                        VAL_MASK_DIR,\n",
    "#                                        BATCH_SIZE,\n",
    "#                                        train_transfrom,\n",
    "#                                        val_transfrom,\n",
    "#                                        NUM_WORKERS,\n",
    "#                                        PIN_MEMORY\n",
    "#                                        )\n",
    "#\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model_unet = UNET_FROM_SCRATCH(in_channels=3, out_channels=1).to(DEVICE)\n",
    "# if LOAD_MODEL:\n",
    "#     load_checkpoint(torch.load('check_unet_scratch.pth.tar'), model_unet)\n",
    "# print_measurements(val_loader, model_unet)\n",
    "# save_predictions_as_imgs(val_loader, model_unet, folder=\"validate_unet/\", device=DEVICE)\n",
    "#\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RESNET_UNET Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [25:46<00:00, 16.45s/it, loss=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing measure\n",
      "Got 76082009/86400000 with acc 88.06\n",
      "Got dice_score:0.890501856803894 \n",
      "Got ioc1:0.445250928401947 \n",
      "Epoch 2 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [25:37<00:00, 16.36s/it, loss=0.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing measure\n",
      "Got 76873843/86400000 with acc 88.97\n",
      "Got dice_score:0.9011495113372803 \n",
      "Got ioc1:0.45057475566864014 \n",
      "Epoch 3 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [25:14<00:00, 16.12s/it, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing measure\n",
      "Got 77310651/86400000 with acc 89.48\n",
      "Got dice_score:0.9044597744941711 \n",
      "Got ioc1:0.45222988724708557 \n",
      "Epoch 4 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [25:42<00:00, 16.41s/it, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing measure\n",
      "Got 77797941/86400000 with acc 90.04\n",
      "Got dice_score:0.9096007943153381 \n",
      "Got ioc1:0.45480039715766907 \n",
      "Epoch 5 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [25:27<00:00, 16.25s/it, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing measure\n",
      "Got 78046376/86400000 with acc 90.33\n",
      "Got dice_score:0.9120803475379944 \n",
      "Got ioc1:0.4560401737689972 \n",
      "Epoch 6 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [25:26<00:00, 16.24s/it, loss=0.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing measure\n",
      "Got 78351778/86400000 with acc 90.68\n",
      "Got dice_score:0.9167390465736389 \n",
      "Got ioc1:0.45836952328681946 \n",
      "Epoch 7 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [25:21<00:00, 16.19s/it, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing measure\n",
      "Got 78356545/86400000 with acc 90.69\n",
      "Got dice_score:0.9151877760887146 \n",
      "Got ioc1:0.4575938880443573 \n",
      "=>saving checkpoint to check_resnet_unet.pth.tar\n",
      "Saving images to.... unet_resnet_saved_photos/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m==\u001B[39m NUM_EPOCHS \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m     21\u001B[0m         save_checkpoint(checkpoint, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcheck_resnet_unet.pth.tar\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 22\u001B[0m         \u001B[43msave_predictions_as_imgs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43munet_resnet_saved_photos/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEnd resnet_unet training\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mE:\\CVDL\\MyApp\\models\\unet_from_scratch\\utils.py:116\u001B[0m, in \u001B[0;36msave_predictions_as_imgs\u001B[1;34m(loader, model, folder, device)\u001B[0m\n\u001B[0;32m    114\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 116\u001B[0m     preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    117\u001B[0m     preds \u001B[38;5;241m=\u001B[39m (preds \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m    118\u001B[0m     torchvision\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39msave_image(preds, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfolder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/prediction_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.png\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mE:\\CVDL\\MyApp\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\CVDL\\MyApp\\models\\unet_from_scratch\\model.py:79\u001B[0m, in \u001B[0;36mTrucnateResNET_UNET.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     77\u001B[0m skip_connections1 \u001B[38;5;241m=\u001B[39m skip_connections1[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mups), \u001B[38;5;241m2\u001B[39m):\n\u001B[1;32m---> 79\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mups\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;66;03m# skip_connection = skip_connections[idx // 2]\u001B[39;00m\n\u001B[0;32m     81\u001B[0m \n\u001B[0;32m     82\u001B[0m     \u001B[38;5;66;03m# if x.shape != skip_connection.shape:\u001B[39;00m\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;66;03m#     x = TF.resize(x, size=skip_connection.shape[2:])\u001B[39;00m\n\u001B[0;32m     84\u001B[0m \n\u001B[0;32m     85\u001B[0m     \u001B[38;5;66;03m########################\u001B[39;00m\n\u001B[0;32m     86\u001B[0m     skip_connection1 \u001B[38;5;241m=\u001B[39m skip_connections1[idx \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m]\n",
      "File \u001B[1;32mE:\\CVDL\\MyApp\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\CVDL\\MyApp\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:956\u001B[0m, in \u001B[0;36mConvTranspose2d.forward\u001B[1;34m(self, input, output_size)\u001B[0m\n\u001B[0;32m    951\u001B[0m num_spatial_dims \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m    952\u001B[0m output_padding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_padding(\n\u001B[0;32m    953\u001B[0m     \u001B[38;5;28minput\u001B[39m, output_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel_size,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m    954\u001B[0m     num_spatial_dims, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m--> 956\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv_transpose2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    957\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    958\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_padding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from models.unet_from_scratch.model import TrucnateResNET_UNET\n",
    "\n",
    "model = TrucnateResNET_UNET().to(DEVICE)\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # applies sigmoid on output\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1} / {NUM_EPOCHS}\")\n",
    "    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
    "\n",
    "    # save model\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict()\n",
    "    }\n",
    "    print_measurements(train_loader, model, device=DEVICE)\n",
    "\n",
    "    # check acc\n",
    "    # print examples\n",
    "    if epoch == NUM_EPOCHS -1:\n",
    "        save_checkpoint(checkpoint, 'check_resnet_unet.pth.tar')\n",
    "        save_predictions_as_imgs(train_loader, model, folder=\"unet_resnet_saved_photos/\", device=DEVICE)\n",
    "print(\"End resnet_unet training\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model_resnet_unet = TrucnateResNET_UNET().to(DEVICE)\n",
    "# if LOAD_MODEL:\n",
    "#     load_checkpoint(torch.load('check_resnet_unet.pth.tar'), model_resnet_unet)\n",
    "# print_measurements(val_loader, model_resnet_unet)\n",
    "# save_predictions_as_imgs(val_loader, model_resnet_unet, folder=\"validate_resnet/\", device=DEVICE)\n",
    "#\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
